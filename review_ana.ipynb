{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "#import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 16:01:00 WARN Utils: Your hostname, TamdeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.100 instead (on interface en0)\n",
      "22/10/07 16:01:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/danie/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/danie/.ivy2/jars\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5b2887ed-f0c2-46b5-b49e-730b6550c55f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 in central\n",
      ":: resolution report :: resolve 90ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5b2887ed-f0c2-46b5-b49e-730b6550c55f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 16:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------+--------------+\n",
      "|        prodcut_name|        reviewerName|             summary|          reviewText|overall|unixReviewTime|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+--------------+\n",
      "|        Parasite Eve|ACEMAN1 \"HERE TO ...|A FANTASTIC HORRO...|Have you ever won...|    5.0|    03-16-2008|\n",
      "|     Resident Evil 2|        B. E Jackson|Want to see blood...|Now THIS is how y...|    5.0|    09-07-2005|\n",
      "|       Mario Kart 64|M. King \"The supe...|       Mario kart 64|This mario kart c...|    4.0|    06-24-2009|\n",
      "|         Silent Hill|            Kevin M.|                Hehe|Even as a 31 year...|    4.0|    01-25-2014|\n",
      "|Nintendo 64 Syste...|                Nate|Best Multiplayer ...|Remember getting ...|    5.0|    01-09-2013|\n",
      "|    Mortal Kombat II|David \"Black Wido...|       It has Begun.|MK 2 Rocks, this ...|    5.0|    05-10-2006|\n",
      "|   Super Mario Bros.|           S. Rhodes|                WOW!|Many a gamer will...|    5.0|    09-22-2003|\n",
      "|Namco Ace Combat ...|DB \"aspiring part...|The best flight s...|It really is nice...|    5.0|    01-30-2007|\n",
      "|       Metroid Prime|         John Whaley|   The Hunter Reborn|For those who lov...|    5.0|    06-19-2003|\n",
      "|          B00006IR62|        {HB}SharkMan|Highly Addictive ...|I played this gam...|    5.0|    04-11-2005|\n",
      "|Enter the Matrix ...|Westley Merlin \"W...|This will be the ...|Or on any platfor...|    5.0|    05-06-2003|\n",
      "|Resident Evil Cod...|         Ben Lehbert|Great game but so...|This game was gre...|    3.0|    03-06-2008|\n",
      "|Batman: Arkham As...|Steven M. Shearer...|Best game of year...|not a big batman ...|    5.0|    09-20-2009|\n",
      "|          B00269QLH4|     Chris Longhurst|Like Call Of Duty...|Call Of Duty : Mo...|    4.0|    12-14-2009|\n",
      "|Burnout Revenge -...|N. Durham \"Big Evil\"|A worthy follow u...|Burnout 3 was und...|    4.0|    08-31-2006|\n",
      "|F.E.A.R. First En...|       Laura Lambert|A surprisingly go...|Despite all the g...|    5.0|    03-04-2012|\n",
      "|Microsoft Xbox 36...|Mathew Reuther \"a...|Works perfectly a...|Plugged in, softw...|    5.0|    11-08-2013|\n",
      "|Xbox 360 Wireless...|   ACoss72 \"ACoss72\"|               Solid|Four stars becaus...|    4.0|    01-15-2014|\n",
      "|Lego Star Wars: T...|     Amazon Customer|When you don't wa...|I have never seen...|    1.0|    05-22-2011|\n",
      "|Lego Star Wars: T...|     Margaux Paschke|THE BEST GAME FOR...|This is my son's ...|    4.0|    12-09-2009|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('inreality').config('spark.jars.packages','com.microsoft.azure:spark-mssql-connector_2.12:1.2.0').getOrCreate()\n",
    "\n",
    "server_name = \"jdbc:sqlserver://inreality.database.windows.net:1433\"\n",
    "database_name = \"data\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"dbo.review\"\n",
    "username = \"\"\n",
    "password = \"\" # Please specify password here\n",
    "\n",
    "jdbcDF = spark.read \\\n",
    "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "short_df = jdbcDF.select('prodcut_name','reviewText','summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/danie/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m yes \u001b[38;5;241m=\u001b[39m panda_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m yes]\n\u001b[0;32m----> 9\u001b[0m tagged_texts \u001b[38;5;241m=\u001b[39m pos_tag_sents(\u001b[38;5;28mmap\u001b[39m(word_tokenize, \u001b[38;5;28mlist\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tag/__init__.py:183\u001b[0m, in \u001b[0;36mpos_tag_sents\u001b[0;34m(sentences, tagset, lang)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag_sents\u001b[39m(sentences, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    170\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to tag the\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m    given list of sentences, each consisting of a list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m    :rtype: list(list(tuple(str, str)))\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m [_pos_tag(sent, tagset, tagger, lang) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/danie/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag_sents\n",
    " #   return list_words\n",
    "panda_df = short_df.toPandas()\n",
    "yes = panda_df['reviewText'].tolist()\n",
    "list = [str(i) for i in yes]\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m panda_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tagged_texts\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinding_adj\u001b[39m(text):\n\u001b[1;32m      3\u001b[0m     adj_list \u001b[38;5;241m=\u001b[39m [word_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word_pair \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m word_pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m TextBlob(word_pair[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msentiment[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m TextBlob(word_pair[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msentiment[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_texts' is not defined"
     ]
    }
   ],
   "source": [
    "panda_df['pos'] = tagged_texts\n",
    "def finding_adj(text):\n",
    "    adj_list = [word_pair[0] for word_pair in text if word_pair[1] == 'JJ' and TextBlob(word_pair[0]).sentiment[0] != 0 and TextBlob(word_pair[0]).sentiment[1] != 0]\n",
    "    return adj_list\n",
    "panda_df['adj'] = panda_df['pos'].apply(finding_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "adj_df = panda_df[['prodcut_name','adj']]\n",
    "# We want to the prouct name to be a particular column, so we need to use index=False to avoid turing them into column\n",
    "adj_groupby_df = adj_df.groupby('prodcut_name',as_index=False).sum()\n",
    "\n",
    "def word_cleansing(wordlist):\n",
    "    new_wordlist = [re.sub(\"[+.-]\",'',word.lower()) for word in wordlist]\n",
    "    return new_wordlist\n",
    "adj_groupby_df['adj'] = adj_groupby_df['adj'].apply(word_cleansing)\n",
    "display(adj_groupby_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def reduce_map_on_counting_word_appearance(wordlist):\n",
    "    order_dict = {}\n",
    "    for word in wordlist:\n",
    "        if word not in order_dict.keys():\n",
    "            order_dict[word] = 1 \n",
    "        else:\n",
    "            order_dict[word] += 1\n",
    "    count_desc_dict = sorted(order_dict.items(),key=lambda x:x[1],reverse=True)\n",
    "    return count_desc_dict[:10]\n",
    "\n",
    "def getting_word(text):\n",
    "    return text[0]\n",
    "def getting_number(text):\n",
    "    return text[1]\n",
    "\n",
    "adj_groupby_df['adj_count'] = adj_groupby_df['adj'].apply(reduce_map_on_counting_word_appearance)\n",
    "final_adj_group_df = adj_groupby_df[['prodcut_name','adj_count']]\n",
    "final_adj_explode_df = final_adj_group_df.explode('adj_count')\n",
    "final_adj_explode_df['word'] = final_adj_explode_df['adj_count'].apply(getting_word)\n",
    "final_adj_explode_df['appearance'] = final_adj_explode_df['adj_count'].apply(getting_number)\n",
    "final_adj_df = final_adj_explode_df.drop(columns=['adj_count'],axis=1)\n",
    "final_adj_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df_spark = spark.createDataFrame(final_adj_df)\n",
    "adj_df_table_name = 'dbo.game_adj_top_5'\n",
    "adj_df_spark.write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", adj_df_table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ploarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    plority = blob.sentiment[0]\n",
    "    return plority \n",
    "spark_df = short_df.toPandas()\n",
    "spark_df['score'] = spark_df['reviewText'].apply(get_ploarity)\n",
    "group_df = spark_df.groupby('prodcut_name',as_index=False).mean().sort_values(by='score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_spark = spark.createDataFrame(group_df)\n",
    "group_df_table_name = 'dbo.product_polarity_score'\n",
    "group_df_spark.write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", group_df_table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
